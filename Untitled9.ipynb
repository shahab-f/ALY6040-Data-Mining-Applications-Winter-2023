{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNH7V2FGRVVAOuvCGKdH2Pn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahab-f/ALY6040-Data-Mining-Applications-Winter-2023/blob/main/Untitled9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "l5y8S36oZXWd",
        "outputId": "d6137610-7209-4c8b-e6ce-52eb293aa5f1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TimeoutException",
          "evalue": "Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ac90ae80b982>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Retrieve the API key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HF_TOKEN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Use the API key as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     64\u001b[0m     )\n\u001b[1;32m     65\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTimeoutException\u001b[0m: Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI."
          ]
        }
      ],
      "source": [
        "# Install necessary packages\n",
        "!pip install transformers torch llama-toolchain pexpect\n",
        "\n",
        "import subprocess\n",
        "import pexpect\n",
        "import sys\n",
        "import time\n",
        "from transformers import LlamaTokenizer, LlamaForSequenceClassification\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "\n",
        "'''\n",
        "# List available Llama models (if necessary)\n",
        "result = subprocess.run([\"llama\", \"model\", \"list\", \"--show-all\"], capture_output=True)\n",
        "print(result.stdout)\n",
        "\n",
        "# Download the Llama model using pexpect\n",
        "command = \"llama download --source meta --model-id Meta-Llama3.1-8B\"\n",
        "signed_url = \"https://llama3-1.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiY3loeWVmaDcxYmtocmh3aTZrNm5uMXV4IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvbGxhbWEzLTEubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNDQ0NzgxNH19fV19&Signature=Z3eE%7EpViWe-HZEMK%7EMhLimbvc3lsjeGVO%7EfPsEyFsh6XVtlik%7E09K4eB71zGkP6fCbvipV3isPP6%7E0FV8NGLRUF0VhXD6X2l1siNU59wUxSK%7Ewe-VK%7EGyPlkBqldO6mx6bKkc9vqo76N8yRUIPykgZy9--OnRfp3%7Ej4q%7EZi%7EdtDOAP7%7EV4Hh6iNaX%7EUdH127h6WSsMXdMlHmGQuaVT2I3UTPfJF3y5MiW0DqkH6j%7EPKRfL2Ir-mGGFZ%7EOr9462Pnr9DarYpe6E%7EI6AqfVMKcvCq5q79DVI88IUTmVTOHFIzwF5XSz7Me2YHx1e1iyULPvgHQaYLA1A8G8vxOBCwb1w__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1189374545713921\"  # Replace with your actual signed URL\n",
        "\n",
        "# Spawn the process with a longer timeout\n",
        "child = pexpect.spawn(command, encoding='utf-8', timeout=3600)  # 1 hour timeout\n",
        "\n",
        "# Expect the prompt for the URL and send the URL\n",
        "child.expect(\"https://llama3-1.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiY3loeWVmaDcxYmtocmh3aTZrNm5uMXV4IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvbGxhbWEzLTEubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNDQ0NzgxNH19fV19&Signature=Z3eE%7EpViWe-HZEMK%7EMhLimbvc3lsjeGVO%7EfPsEyFsh6XVtlik%7E09K4eB71zGkP6fCbvipV3isPP6%7E0FV8NGLRUF0VhXD6X2l1siNU59wUxSK%7Ewe-VK%7EGyPlkBqldO6mx6bKkc9vqo76N8yRUIPykgZy9--OnRfp3%7Ej4q%7EZi%7EdtDOAP7%7EV4Hh6iNaX%7EUdH127h6WSsMXdMlHmGQuaVT2I3UTPfJF3y5MiW0DqkH6j%7EPKRfL2Ir-mGGFZ%7EOr9462Pnr9DarYpe6E%7EI6AqfVMKcvCq5q79DVI88IUTmVTOHFIzwF5XSz7Me2YHx1e1iyULPvgHQaYLA1A8G8vxOBCwb1w__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1189374545713921\")\n",
        "child.sendline(signed_url)\n",
        "\n",
        "# Function to handle output in real-time\n",
        "def print_output(child):\n",
        "    while True:\n",
        "        try:\n",
        "            line = child.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            print(line, end='', flush=True)\n",
        "        except pexpect.exceptions.TIMEOUT:\n",
        "            print(\".\", end='', flush=True)  # Print a dot to show it's still alive\n",
        "            continue\n",
        "        except pexpect.exceptions.EOF:\n",
        "            break\n",
        "\n",
        "# Print the output in real-time\n",
        "try:\n",
        "    print_output(child)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nProcess interrupted by user. Terminating...\")\n",
        "    child.terminate(force=True)\n",
        "\n",
        "# Wait for the process to complete\n",
        "try:\n",
        "    child.expect(pexpect.EOF, timeout=None)\n",
        "except pexpect.exceptions.TIMEOUT:\n",
        "    print(\"\\nProcess timed out. It may still be running in the background.\")\n",
        "except pexpect.exceptions.EOF:\n",
        "    print(\"\\nProcess completed.\")\n",
        "\n",
        "# Print any remaining output\n",
        "print(child.before)\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve the API key\n",
        "api_key = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Use the API key as needed\n",
        "# For example, with the Hugging Face Hub:\n",
        "from huggingface_hub import login\n",
        "login(token=api_key)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "\n",
        "# Login using the token\n",
        "login(token=os.environ[\"HF_TOKEN\"])\n",
        "\n",
        "# Now load the model\n",
        "from transformers import LlamaTokenizer, LlamaForSequenceClassification\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "model = LlamaForSequenceClassification.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load the pre-trained Llama model and tokenizer\n",
        "tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "model = LlamaForSequenceClassification.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "\n",
        "# Function to tokenize and process documents\n",
        "def process_documents(documents):\n",
        "    inputs = tokenizer(documents, return_tensors='pt', padding=True, truncation=True)\n",
        "    outputs = model(**inputs)\n",
        "    return outputs\n",
        "\n",
        "# Function to extract themes and trends\n",
        "def extract_themes_and_trends(documents):\n",
        "    outputs = process_documents(documents)\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "    themes = defaultdict(list)\n",
        "    for doc_id, prob in enumerate(probabilities):\n",
        "        if prob[0] > 0.5:  # Assuming first class is 'Theme A'\n",
        "            themes['Theme A'].append(documents[doc_id])\n",
        "        elif prob[1] > 0.5:  # Assuming second class is 'Theme B'\n",
        "            themes['Theme B'].append(documents[doc_id])\n",
        "\n",
        "    return themes\n",
        "\n",
        "# Function to compare documents side-by-side\n",
        "def compare_documents(doc1, doc2):\n",
        "    inputs = tokenizer([doc1, doc2], return_tensors='pt', padding=True, truncation=True)\n",
        "    outputs = model(**inputs)\n",
        "    similarities = torch.cosine_similarity(outputs[0], outputs[1], dim=0)\n",
        "    return similarities\n",
        "\n",
        "# Example documents\n",
        "documents = [\n",
        "    \"This is the first legal document discussing case law A.\",\n",
        "    \"This is the second legal document referring to case law B.\",\n",
        "    \"Another document mentions case law A and its implications.\"\n",
        "]\n",
        "\n",
        "# Analyze documents to extract themes and trends\n",
        "themes_and_trends = extract_themes_and_trends(documents)\n",
        "print(\"Extracted Themes and Trends:\", themes_and_trends)\n",
        "\n",
        "# Compare two documents\n",
        "similarity_score = compare_documents(documents[0], documents[1])\n",
        "print(\"Similarity Score between Document 1 and Document 2:\", similarity_score)\n",
        "\n",
        "# Function to generate a report\n",
        "def generate_report(themes_and_trends, similarity_score):\n",
        "    report = f\"Legal Research Report\\n\\nThemes and Trends:\\n{themes_and_trends}\\n\\n\"\n",
        "    report += f\"Similarity Score between Document 1 and Document 2: {similarity_score.item()}\\n\"\n",
        "    return report\n",
        "\n",
        "# Generate and print the report\n",
        "report = generate_report(themes_and_trends, similarity_score)\n",
        "print(report)\n"
      ]
    }
  ]
}